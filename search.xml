<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[linux三剑客—awk]]></title>
    <url>%2F2020%2F01%2F23%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AALinux%E5%91%BD%E4%BB%A4%2Flinux%E4%B8%89%E5%89%91%E5%AE%A2%E2%80%94awk%2F</url>
    <content type="text"><![CDATA[未完待续]]></content>
      <categories>
        <category>每天一个Linux命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux三剑客—grep]]></title>
    <url>%2F2020%2F01%2F17%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AALinux%E5%91%BD%E4%BB%A4%2Flinux%E4%B8%89%E5%89%91%E5%AE%A2%E2%80%94grep%2F</url>
    <content type="text"><![CDATA[写在前面 awk、grep、sed是linux操作文本的三大利器，合称文本三剑客。掌握其中两个基本上就可以横着走了，掌握三个更是江湖无敌了。 其中，grep适合单纯的查找或匹配文本，sed更适合编辑匹配到的文本，awk适合格式化文本，对文本进行较复杂格式处理。 如何使用 这个可以通过 man grep 查看详情。 命令格式 grep [option] pattern file 命令参数 这里我就不全介绍了。只列出常用的。 -A&lt;显示行数&gt;：除了显示符合范本样式的那一列之外，并显示该行之后的内容。 after -B&lt;显示行数&gt;：除了显示符合样式的那一行之外，并显示该行之前的内容。before -C&lt;显示行数&gt;：除了显示符合样式的那一行之外，并显示该行之前后的内容。 -c：统计匹配的行数 count -e ：实现多个选项间的逻辑or 关系 -f FILE：将PATTERN匹配模式写在文件里用 -i --ignore-case #忽略字符大小写的差别。 -n：显示匹配的行号 -o：仅显示匹配到的字符串 不显示其他内容 -v：显示不被pattern 匹配到的行，相当于[^] 反向匹配 -w：匹配 整个单词。比如一行中有“hello”，如果只用“hell”就匹配不到 -x：跟-w差不多意思，匹配整行。如果一行里有空格，也需要在pattern里体现出来。 -l：显示匹配到的文件 -L：和-l相反，如果显示没有pattern的文件。 实战应用 先生成一个文件吧，就直接用下面这个来举例了： man grep &gt; grep.txt after before central -c count 只计数 -o -w 与 -x 1.txt里什么都没有。如果一个项目里有非常多的源码，自己记得某行代码但是忘了在哪个文件里，这个命令就非常有用了。 有一点需要注意，就是在’'里面第一个‘-’会报错，这里需要加上一个转义符。]]></content>
      <categories>
        <category>每天一个Linux命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux三剑客—sed]]></title>
    <url>%2F2020%2F01%2F17%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AALinux%E5%91%BD%E4%BB%A4%2Flinux%E4%B8%89%E5%89%91%E5%AE%A2%E2%80%94sed%2F</url>
    <content type="text"><![CDATA[认识sed sed 是一种流编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（patternspace ），接着用sed 命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。然后读入下行，执行下一个循环。如果没有使诸如‘D’ 的特殊命令，那会在两个循环之间清空模式空间，但不会清空保留空间。这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出或-i。 功能：主要用来自动编辑一个或多个文件, 简化对文件的反复操作 如何使用 命令格式 sed [options] ‘[地址定界] command’ file(s) 注意，这里的options和command是两个物种。command才是对每行内容进行操作的命令，而options可以看做是如何处理最终的成品。 常用选项options -n：不输出模式空间内容到屏幕，即不自动打印，只打印匹配到的行 -e：多点编辑，对每行处理时，可以有多个Script -f：把Script写到文件当中，在执行sed时-f 指定文件路径，如果是多个Script，换行写 -r：支持扩展的正则表达式 -i：直接将处理的结果写入文件 -i.bak：在将处理的结果写入文件之前备份一份 地址定界 举个例子就知道了，看下表，表格来自linux三剑客之sed命令精讲-陈思齐 地址范围 含义 10{sed-commands} 对第10行操作 10，20{sed-commands} 对10到20行操作，包括第10，20行 10，+20{sed-commands} 对10到30（10+20）行操作，包括第10，30行 1～2{sed-commands} 对1，3，5，7…行操作 10，${sed-commands} 对10到最后一行（$代表最后一行）操作，包括第10行 /chensiqi/{sed-commands} 对匹配chensiqi的行操作 /chensiqi/,/Alex/{sed-commands} 对匹配chensiqi的行到匹配Alex的行操作 /chensiqi/,${sed-commands} 对匹配chensiqi的行到最后一行操作 /chensiqi/,10{sed-commands} 对匹配chensiqi的行到第10行操作，注意：如果前10行没有匹配到chensiqi，sed软件会显示10行以后的匹配chensiqi的行 1,/Alex/{sed-commands} 对第1行到匹配Alex的行操作 /chensiqi/,+2{sed-commands} 对匹配chensiqi的行到其后的2行操作 编辑命令command d：删除模式空间匹配的行，并立即启用下一轮循环 p：打印当前模式空间内容，追加到默认输出之后 a：在指定行后面追加文本，支持使用\n实现多行追加 i：在行前面插入文本，支持使用\n实现多行追加 c：替换行为单行或多行文本，支持使用\n实现多行追加 w：保存模式匹配的行至指定文件 r：读取指定文件的文本至模式空间中匹配到的行后 =：为模式空间中的行打印行号 !：模式空间中匹配行取反处理 s///：查找替换，支持使用其它分隔符，如：s@@@，s###； 加g表示行内全局替换； 在替换时，可以加一下命令，实现大小写转换 \l：把下个字符转换成小写。 \L：把replacement字母转换成小写，直到\U或\E出现。 \u：把下个字符转换成大写。 \U：把replacement字母转换成大写，直到\L或\E出现。 \E：停止以\L或\U开始的大小写转换 实战应用 文本操作无非就是增删改查这老四样。每条命令不一一细说了，从应用的角度来掌握sed命令。 查 查找指定行 sed -nr ‘2p’ sed.txt # 查找第二行，-n表示打印查找的内容，-r表示使用正则匹配。 -nr最好都加上，因为有的时候不加-r会出问题。 sed -nr ‘2,10p’ sed.txt # 2-16行 sed -nr ‘/a/,16p’ sed.txt # 这里需要注意 这里本来意思是查找含a的行一直到第16行，但是如果第16行之外还有a，则不会在第16行停止，会包含后面的行。所以查找的时候请不要将正则查找应用于限制行数的场景。 sed -nr “/a|y|2/p” sed.txt # 查找含有a或者2或者y的行 sed -nr ‘/a/{/c/p}’ sed.txt # 匹配同时含有a和c的内容，话说我也不知道为啥要把p放到大括号里面去。。 sed -nr ‘/a/{/b/{/c/p}}’ sed.txt # 匹配同时含有a和b和c的内容。。。 sed -nr ‘1~2p’ sed.txt # 奇数行 sed -nr ‘2~2p’ sed.txt # 偶数行 sed -nr ‘/[1]*$/p’ sed.txt # 查找所有空行 增 增不过是将查到的位置处增加指定内容而已，搞清楚了查，这个就很简单了。只有批量操作的时候才会用到这个，如果只是简单增加一两行，还是文本编辑器打开文件直接修改来的方便。 sed -r ‘2~2i insert a new line’ sed.txt #在之前偶数行的位置增加新内容 sed -r ‘2~2a add a new line’ sed.txt #在之前偶数行的位置后面一行增加新内容 删 删只需要把p换成d就可以了。 改 整行替换 c：change sed -r ‘2c jkjkljasd’ sed.txt # 跟增差不都，只需要把i或者a换成c就行了。 除了整行change之外，sed还有一个大招，就是正则匹配替换。这个是重点。 正则匹配替换 语法： sed -i ‘s/目标内容/替换内容/g’ file(s) sed -i ‘s#目标内容#替换内容#g’ file(s) 定界符/或#都行，第一个和第二个之间的就是被替换的内容，第二个和第三个之间的就是替换后的内容。 “目标内容”能用正则表达式，但替换内容不能用，必须是具体的。因为替换内容使用正则的话会让sed软件无所适从，它不知道你要替换什么内容。 g代表全局，没有g则表示每行匹配到的第一个位置进行替换，其他位置保持不变。 sed -r ‘s/a/AA/’ sed.txt # 每行第一处a替换为AA。 sed -r ‘s#a#AA#g’ sed.txt # 将所有的a替换为AA。 sed ‘3s#0#9#’ sed.txt #只把第三行第一个0替换成9 变量替换 看下面两个例子。有两点要注意，单引号双引号是不一样的；还可以进行变量替换。 12345678910111213yang@yang ~&gt; sed '3s#0#$PATH#' sed.txt012$PATH09000……yang@yang ~&gt; sed "3s#0#$PATH#" sed.txt012/home/yang/anaconda3/condabin /home/yang/anaconda3/bin /usr/local/sbin /usr/local/bin /usr/sbin /usr/bin /sbin /bin /usr/games /usr/local/games09000…… 分组替换()和\1的使用说明 sed的()的功能可以记住正则表达式的一部分，其中，\1为第一个记住的模式即第一个小括号中的匹配内容，\2第二个记住的模式，即第二个小括号中的匹配内容，sed最多可以记住9个。 123456789yang@yang ~&gt; cat sed.txtname,age,sex,idyang,19,male,999liou,90,famale,89yang@yang ~&gt; sed -r 's#(.*),(.*),(.*),(.*)#&amp; ------ \4#' sed.txtname,age,sex,id ------ idyang,19,male,999 ------ 999liou,90,famale,89 ------ 89 这个分组匹配还是十分强大十分有用的，对格式化的数据比如日志、训练数据什么的，想要取出某一列非常有效。 特殊字符说明： &amp; ： 表示匹配的要替换的内容。 最后来看一个例子： \t ↩︎]]></content>
      <categories>
        <category>每天一个Linux命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[find命令]]></title>
    <url>%2F2020%2F01%2F05%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AALinux%E5%91%BD%E4%BB%A4%2Ffind%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux find命令 语法 find path -option 时间戳 atime、ctime与mtime（小时） atime是指access time，即文件被读取或者执行的时间，修改文件是不会改变access time的。 ctime即change time文件状态改变时间，指文件的i结点被修改的时间，如通过chmod修改文件属性，ctime就会被修改。 mtime即modify time，指文件内容被修改的时间。 amin/cmin/mmin选项，根据时间查找(分钟) 1find . –mtime n # n指的是n天, +n、-n、n分别表示： +n:大于n -n:小于n n:等于n 常用示例 查找大小在50MB到100MB之间 find . -size +50M -size -100M 使用名称和忽略案例查找文件 find . -iname test 查找777个权限的文件 find . -type f ! -perm 777 查找可执行文件 find . -perm a=x 查找所有空目录、空文件 find . -type d -empty find . -type f -empty]]></content>
      <categories>
        <category>每天一个Linux命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[磁盘永久挂载]]></title>
    <url>%2F2019%2F12%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AALinux%E5%91%BD%E4%BB%A4%2F%E7%A3%81%E7%9B%98%E6%B0%B8%E4%B9%85%E6%8C%82%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[磁盘挂载 1、查看Linux硬盘信息 sudo fdisk -l df -h 2、挂载分区 sudo mount /dev/sdb1 /data 3、查看磁盘分区的UUID sudo blkid 4、配置开机自动挂载 sudo vim /etc/fstab 文件末尾加上： UUID=11263962-9715-473f-9421-0b604e895aaa /data ext4 defaults 0 2]]></content>
      <categories>
        <category>每天一个Linux命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测中的一些疑惑]]></title>
    <url>%2F2019%2F12%2F17%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E7%96%91%E6%83%91%2F</url>
    <content type="text"><![CDATA[YOLO系列问题 1、YOLOV3中targets如何设置？shape是多少？ 我们知道一个每个box只需要4个值表示坐标，一个值表示类别，那么targets的shape应该是 n*5 ， 但是实际上每一个batch的targets的shape是 n*6，n表示这个batch中的目标个数，6个数中后4位是x,y,w,h，第2位是该目标所属的类别index，（index是从0开始的）。那第一个数是什么呢？看第二个问题。 2、每张图片中目标个数不一样，不同shape的target在一个batch里怎么能放到一起呢？ 在target中增加一位来表示其属于哪一张图片。 比如batch_size设置为4，第一个batch里4张图片中的目标个数分别为4，3，1，2。那么n就是10。这一个batch的targets的shape就是10*6，那这6个数里的第一位就分别是[0,0,0,0,1,1,1,2,3,3]。 3、损失函数如何计算？ YOLOv3的损失函数主要分为三个部分： 目标定位偏移量损失 目标置信度损失 目标分类损失 1、目标定位偏移量损失: 12345mse_loss = nn.MSELoss()loss_x = mse_loss(x[obj_mask], tx[obj_mask])loss_y = mse_loss(y[obj_mask], ty[obj_mask])loss_w = mse_loss(w[obj_mask], tw[obj_mask])loss_h = mse_loss(h[obj_mask], th[obj_mask]) 这里的x,y,w,h为特征图上目标对应位置的预测值。 tx,ty,tw,th是真实值，那这个值是否是我们标注时的值呢？ 不是！！！ 假设我们先在有一张图片，图片大小是416*416，现在有个待检测目标，其坐标值是xmin,ymin,xmax,ymax=[80,120,300,350]。那么其对应的tx,ty,tw,th怎么计算呢？ 同样的原始图像，在不同的特征图里，tx,ty,tw,th是不一样的。比如特征图大小为3*13*13。3表示有三种大小不同的anchor，13是特征图的长和宽。anchors=[(116, 90), (156, 198), (373, 326)] 第一步：确认使用三个anchor中的哪一个去回归坐标。 x,y,w,h = [190,235,220,230] 真实框和三个anchor做iou，选择最大的那一个anchor去回归。 anchor只有长和宽，并没有x,y，如何计算iou呢？看下面文章。iou分别是： 116*90/220/230=0.2063 156*198/220/230=0.6104 220*230/373/326=0.4161 懒人赏析YOLO V2 anchors里面的（156,198）就负责来回归这个目标。 x,y,w,h = [0.457, 0.565, 0.529, 0.553]]]></content>
      <categories>
        <category>_posts</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[screen]]></title>
    <url>%2F2019%2F12%2F13%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AALinux%E5%91%BD%E4%BB%A4%2Fscreen%2F</url>
    <content type="text"><![CDATA[创建一个会话 screen -S notebook 查看会话列表 screen -ls 从会话终端退出 Ctrl + a + d 结束某个会话 screen -X -S 5541 quit 恢复会话 Screen -r 5541]]></content>
      <categories>
        <category>每天一个Linux命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[偏函数]]></title>
    <url>%2F2019%2F11%2F29%2FPython%E5%9F%BA%E7%A1%80%2F%E5%81%8F%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Python的functools模块提供了很多有用的功能，其中一个就是偏函数（Partial function）。 在介绍函数参数的时候，我们讲到，通过设定参数的默认值，可以降低函数调用的难度。而偏函数也可以做到这一点。举例如下： int()函数可以把字符串转换为整数，当仅传入字符串时，int()函数默认按十进制转换： 12&gt;&gt;&gt; int(&apos;12345&apos;)12345 但int()函数还提供额外的base参数，默认值为10。如果传入base参数，就可以做N进制的转换： 1234&gt;&gt;&gt; int(&apos;12345&apos;, base=8)5349&gt;&gt;&gt; int(&apos;12345&apos;, 16)74565 假设要转换大量的二进制字符串，每次都传入int(x, base=2)非常麻烦，于是，我们想到，可以定义一个int2()的函数，默认把base=2传进去： 12def int2(x, base=2): return int(x, base) 这样，我们转换二进制就非常方便了： 1234&gt;&gt;&gt; int2(&apos;1000000&apos;)64&gt;&gt;&gt; int2(&apos;1010101&apos;)85 functools.partial就是帮助我们创建一个偏函数的，不需要我们自己定义int2()，可以直接使用下面的代码创建一个新的函数int2： 123456&gt;&gt;&gt; import functools&gt;&gt;&gt; int2 = functools.partial(int, base=2)&gt;&gt;&gt; int2(&apos;1000000&apos;)64&gt;&gt;&gt; int2(&apos;1010101&apos;)85 所以，简单总结functools.partial的作用就是，把一个函数的某些参数给固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。 注意到上面的新的int2函数，仅仅是把base参数重新设定默认值为2，但也可以在函数调用时传入其他值： 12&gt;&gt;&gt; int2(&apos;1000000&apos;, base=10)1000000 最后，创建偏函数时，实际上可以接收函数对象、*args和**kw这3个参数，当传入： 1int2 = functools.partial(int, base=2) 实际上固定了int()函数的关键字参数base，也就是： 1int2(&apos;10010&apos;) 相当于： 12kw = &#123; &apos;base&apos;: 2 &#125;int(&apos;10010&apos;, **kw) 当传入： 1max2 = functools.partial(max, 10) 实际上会把10作为*args的一部分自动加到左边，也就是： 1max2(5, 6, 7) 相当于： 12args = (10, 5, 6, 7)max(*args) 结果为10。 小结 当函数的参数个数太多，需要简化时，使用functools.partial可以创建一个新的函数，这个新函数可以固定住原函数的部分参数，从而在调用时更简单。 参考资料-廖雪峰]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Squeeze-and-Excitation]]></title>
    <url>%2F2019%2F11%2F29%2F%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2FSqueeze-and-Excitation%2F</url>
    <content type="text"><![CDATA[基本信息 https://arxiv.org/abs/1709.01507 题目:Squeeze-and-Excitation Networks 作者:Jie Hu 时间:2017]]></content>
      <categories>
        <category>论文学习笔记与代码实现</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mobilenetv2]]></title>
    <url>%2F2019%2F11%2F29%2F%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2FMobilenetv2%2F</url>
    <content type="text"><![CDATA[基本信息 https://arxiv.org/abs/1801.04381 题目:MobileNetV2: Inverted Residuals and Linear Bottlenecks 作者:Mark Sandler(google) 时间:2018 主要内容]]></content>
      <categories>
        <category>论文学习笔记与代码实现</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Swish激活函数]]></title>
    <url>%2F2019%2F11%2F29%2F%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2FSwish%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[基本信息 https://arxiv.org/abs/1710.05941 题目:Searching for Activation Functions 作者:Google Brain 时间:2017 主要内容 自ReLU提出和应用在深度学习模型后，其有效性和简洁性使得大家一直在使用。 作者想通过实验和搜索网络来自动的寻找一种更高效的激活函数。（也只有谷歌等大厂有钱有卡有数据才玩得转）。最后通过实验证明，他们确实找到了一种更高效的激活函数，他们命名为Swish。其函数如下： f(x) = x · sigmoid(βx) 直观图看上去是这样的： 代码实现 1、简洁版 123class Swish(nn.Module): def forward(self, x): return x * torch.sigmoid(x) 2、内存优化版 1234567891011121314151617class SwishImplementation(torch.autograd.Function): @staticmethod def forward(ctx, i): result = i * torch.sigmoid(i) ctx.save_for_backward(i) return result @staticmethod def backward(ctx, grad_output): i = ctx.saved_variables[0] sigmoid_i = torch.sigmoid(i) return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))class MemoryEfficientSwish(nn.Module): def forward(self, x): return SwishImplementation.apply(x) 以上代码实现来自https://github.com/lukemelas/EfficientNet-PyTorch 使用的时候直接用Swish()或者MemoryEfficientSwish()替代torch.nn.ReLU(). Swish在做预测推理的时候用，MemoryEfficientSwish在训练的时候用。]]></content>
      <categories>
        <category>论文学习笔记与代码实现</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[EfficientNet以及Pytorch实现]]></title>
    <url>%2F2019%2F11%2F29%2F%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2FEfficientNet%E4%BB%A5%E5%8F%8APytorch%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[基本信息 https://arxiv.org/abs/1905.11946 题目:EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks 作者:谷歌 Mingxing Tan 1 Quoc V. Le 1 时间:2019 主要内容 现在的深度学习发展，想要提高模型精度，有三种主流方向： 更深的深度 更宽的宽度 更高的输入图像分辨率 以上三条都能提升模型的精度，但是随之而来的问题是模型越来越多，参数量越来越多。 而且精度提高到一定的程度后，再增大模型的性价比非常低。比如文章提到ResNet-1000在深度上是ResNet-101的10倍，但是二者精度差不多，提升非常小。 作者想找到width、depth、和resolution scaling三者之间的一种权衡，在固定的计算资源（参数量和浮点计算量）下，怎样达到最好的模型效果。 先搭建一个base网络EfﬁcientNet-B0，base网络主要用到的是MBConv模块。 另外一些实现细节： 用到了Squeeze-and-Excitation Networks 使用Swish替换了ReLU。 模型结构如下： 然后在计算资源限制的情况下最大化准确率。 暴力美学，接着就是用卡跑出来的结果 α = 1.2, β = 1.1, γ = 1.15。 接着就是固定上面三个值，通过不同的φ来实现B1-B7网络（目前官方还给出了B8模型）。 这里我有点困惑，我们来看下B7中的三个参数，width_coefficient=2.0，depth_coefficient=3.1，resolution=600/224=2.68。 按照论文的说法，固定α = 1.2，那么由depth_coefficient=3.1和下面的公式 求得φ = 6.2。 但是同样的我们由width_coefficient和β计算的φ = 7.3，由resolution和γ计算的φ = 7.05。这三个值并不相同。这与文章的内容有出入。]]></content>
      <categories>
        <category>论文学习笔记与代码实现</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
        <tag>Deep Learning</tag>
        <tag>EfficientNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[损失函数合集]]></title>
    <url>%2F2019%2F11%2F20%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%B3%BB%E5%88%97%2F%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86%2F</url>
    <content type="text"><![CDATA[最常用的6种损失函数 很多的 loss 函数都有 size_average、reduce 和reduction三个参数. 现在前两个官方已经不推荐使用了. 一般损失函数都是直接计算 batch 的数据，因此返回的 loss 结果都是维度为 (batch_size, ) 的向量。 所以可以通过参数进行结果控制. reduction 的取值为 ‘none’ | ‘mean’ | ‘sum’. 默认为mean “none” 表示不做处理 “sum” 表示取batch_size的和 “mean” 表示取batch_size的均值 1、L1范数损失 L1Loss 计算 output 和 target 之差的绝对值。 12345678910111213141516171819import torchimport torch.nn as nnloss = nn.L1Loss(reduction="none")input_ = torch.randn(1, 2)target = torch.randn(1, 2)output = loss(input_, target)print(input_)print(target)print(output)print(nn.L1Loss(reduction="sum")(input_, target))print(nn.L1Loss()(input_, target))###结果为:tensor([[-1.1805, -1.0040]])tensor([[ 0.6205, -0.3034]])tensor([[1.8010, 0.7006]])tensor(2.5016)tensor(1.2508) 2、均方误差损失 MSELoss 计算 output 和 target 之差的均方差。 12345678910111213141516171819import torchimport torch.nn as nnloss = nn.MSELoss(reduction="none")input_ = torch.randn(1, 2)target = torch.randn(1, 2)output = loss(input_, target)print(input_)print(target)print(output)print(nn.MSELoss(reduction="sum")(input_, target))print(nn.MSELoss()(input_, target))###结果为:tensor([[-1.5865, 0.7328]])tensor([[-0.1690, -0.1935]])tensor([[2.0092, 0.8581]])tensor(2.8673)tensor(1.4336) 3、交叉熵损失 CrossEntropyLoss 当训练有 C 个类别的分类问题时很有效. 可选参数 weight 必须是一个1维 Tensor, 权重将被分配给各个类别. 对于不平衡的训练集非常有效。 在多分类任务中，经常采用 softmax 激活函数+交叉熵损失函数，因为交叉熵描述了两个概率分布的差异，然而神经网络输出的是向量，并不是概率分布的形式。所以需要 softmax激活函数将一个向量进行“归一化”成概率分布的形式，再采用交叉熵损失函数计算 loss。 假设batch_size为2,对于一个二分类模型,最终模型输出output,标签为target,则cost计算方式如下: 1234567891011121314151617181920212223import torchimport torch.nn as nnloss = nn.CrossEntropyLoss()output = torch.tensor([[-2.9, 1.3], [0.3, 0.4] ])target = torch.LongTensor([1, 0])cost = loss(output, target)print(output)print(target)print(cost)###结果为:# tensor([[-2.9000, 1.3000],# [ 0.3000, 0.4000]])# tensor([1, 0])# tensor(0.3796)(-1*math.log(math.exp(1.3)/(math.exp(-2.9)+math.exp(1.3)))-1*math.log(math.exp(0.3)/(math.exp(0.3)+math.exp(0.4))))/2# 0.3796404573727445 4、二进制交叉熵损失 BCELoss 二分类或者多标签二分类时会用到。 target为0或1。 output取值在0——1之间。（常用sigmoid使范围变成0——1） 1234567891011121314import torchimport torch.nn as nnloss = nn.BCELoss()output = torch.tensor([[0.8, 0.8, 0.7], [0.3, 0.4, 0.9] ])target = torch.Tensor([[1, 1, 0], [0, 1, 1]])cost = loss(output, target)print(cost)# tensor(0.5048)# -1*(math.log(0.8)+math.log(0.8)+math.log(0.3)+math.log(0.7)+math.log(0.4)+math.log(0.9))/6 5、BCEWithLogitsLoss BCEWithLogitsLoss损失函数只是把 Sigmoid 层集成到了 BCELoss 类中，就不需要自己再添加一层sigmoid。 6、MultiLabelMarginLoss多标签损失函数 功能： 用于一个样本属于多个类别时的分类任务。例如一个四分类任务，样本x属于第0类，第1类，不属于第2类，第3类。 计算公式： 1234567891011121314151617181920import torchimport torch.nn as nnloss = nn.MultiLabelMarginLoss()x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]])# for target y, only consider labels 3 and 0, not after label -1y = torch.LongTensor([[3, 0, -1, 1]])cost = loss(x, y)print(cost)# 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))# tensor(0.8500)y = torch.LongTensor([[3, -1, 2, 1]])print(loss(x,y))# 0.25 * ((1-(0.8-0.1)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))# tensor(0.3250)]]></content>
      <categories>
        <category>深度学习基石系列</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>基石</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cannot run source activate with conda in Fish-shell]]></title>
    <url>%2F2019%2F11%2F15%2Fbugs%2FCannot%20run%20source%20activate%20with%20conda%20in%20Fish-shell%2F</url>
    <content type="text"><![CDATA[fish下conda activate失败解决方案 找到~/.config/fish/config.fish文件,添加以下内容并重启终端. 1source &lt;PATH_TO_ROOT&gt;/etc/fish/conf.d/conda.fish 其中,&lt;PATH_TO_ROOT&gt;是你anaconda的安装位置. 以下命令可查看: 1conda info --root]]></content>
      <categories>
        <category>bugs解决方案</category>
      </categories>
      <tags>
        <tag>fish</tag>
        <tag>conda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2019%2F11%2F14%2FPython%E5%9F%BA%E7%A1%80%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Re模块 re的英文全拼是regular expression. 正则语法 操作符 说明 实例 . 表示任何单个字符 [ ] 字符集， 对单个字符给出取值范围 [abc]表示a、b、c， [a‐z]表示a到z单个字符 [^ ] 非字符集， 对单个字符给出排除范围 [^abc]表示非a或b或c的单个字符 * 前一个字符0次或无限次扩展 abc* 表示 ab、abc、abcc、abccc等 + 前一个字符1次或无限次扩展 abc+ 表示 abc、abcc、abccc等 ? 前一个字符0次或1次扩展 abc? 表示 ab、abc | 左右表达式任意一个 abc | def 表示 abc、def {m} 扩展前一个字符m次 ab{2}c表示abbc {m,n} 扩展前一个字符m至n次（含n） ab{1,2}c表示abc、abbc ^ 匹配字符串开头 ^abc表示abc且在一个字符串的开头 $ 匹配字符串结尾 abc$表示abc且在一个字符串的结尾 ( ) 分组标记， 内部只能使用 | 操作符 (abc)表示abc， (abc | def)表示abc、def \d 数字， 等价于[0‐9] \w 单词字符， 等价于[A‐Za‐z0‐9_] re模块使用方法 12345import retext = ""pattern = re.compile(r"") # 正则语法pattern.search(……) re库的6个主要函数 函数 说明 search() 在一个字符串中搜索匹配正则表达式的第一个位置， 返回match对象 match() 从一个字符串的开始位置起匹配正则表达式， 返回match对象 finditer() 搜索字符串， 返回一个匹配结果的迭代类型， 每个迭代元素是match对象 findall() 搜索字符串， 以列表类型返回全部能匹配的子串列表 split() 将一个字符串按照正则表达式匹配结果进行分割， 返回列表类型 re.sub() 在一个字符串中替换所有匹配正则表达式的子串， 返回替换后的字符串 match对象介绍 Match对象是一次匹配的结果，包含匹配的很多信息 4种主要属性 属性 说明 .string 待匹配的文本(原文,非搜索结果) .re 匹配时使用的patter对象（正则表达式） .pos 正则表达式搜索文本的开始位置(非搜索结果的起始位置) .endpos 正则表达式搜索文本的结束位置 4种方法 方法 说明 .group(0) 获得匹配后的字符串 .start() 匹配字符串在原始字符串的开始位置 .end() 匹配字符串在原始字符串的结束位置 .span() 返回(.start(), .end()) 正则表达式捕获分组 Group group() 方法返回查找的字符串。那怎么得到查找的字符串的某一部分呢？ 比如用正则 d{4}-d{2}-d{2} 提取年月日信息 单独把年，月，日提出来可以用小括号 (d{4})-(d{2})-(d{2}) group(index) 方法可以按照小括号的顺序，依次提取每个分组信息。如果分组较多的话，使用数字不如用名称提取每个分组信息更方便。语法由 (…) 变为 (?P…) 贪婪匹配 123&gt;&gt;&gt; match = re.search(r'PY.*N', 'PYANBNCNDN')&gt;&gt;&gt; match.group(0) 'PYANBNCNDN' 同时匹配长短不同的多项，返回哪一个呢？ Re库默认采用贪婪匹配，即输出匹配最长的子串 最小匹配 如何输出最短的子串呢？ 123&gt;&gt;&gt; match = re.search(r'PY.*?N', 'PYANBNCNDN')&gt;&gt;&gt; match.group(0) 'PYAN' 操作符 说明 *? 前一个字符0次或无限次扩展， 最小匹配 +? 前一个字符1次或无限次扩展， 最小匹配 ?? 前一个字符0次或1次扩展，最小匹配 {m,n}? 扩展前一个字符m至n次（含n）， 最小匹配 只要长度输出可能不同的，都可以通过在操作符后增加?变成最小匹配]]></content>
      <categories>
        <category>Python学习笔记</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git常见操作]]></title>
    <url>%2F2019%2F11%2F05%2Fgit%E6%95%99%E7%A8%8B%2Fgit%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[git status 中文编码问题 git config --global core.quotepath false 查看配置信息 要检查已有的配置信息，可以使用git config —list命令： 123456789101112131415yang@yangdeMacBook-Pro ~/blog&gt; git config --list credential.helper=osxkeychainfilter.lfs.smudge=git-lfs smudge -- %ffilter.lfs.process=git-lfs filter-processfilter.lfs.required=truefilter.lfs.clean=git-lfs clean -- %fuser.name=whuhituser.email=whuhit09@gmail.comcore.excludesfile=/Users/yang/.gitignore_globalcore.quotepath=falsedifftool.sourcetree.cmd=opendiff &quot;$LOCAL&quot; &quot;$REMOTE&quot;difftool.sourcetree.path=mergetool.sourcetree.cmd=/Applications/Sourcetree.app/Contents/Resources/opendiff-w.sh &quot;$LOCAL&quot; &quot;$REMOTE&quot; -ancestor &quot;$BASE&quot; -merge &quot;$MERGED&quot;mergetool.sourcetree.trustexitcode=truegui.encoding=utf-8 查看单条信息 git config user.name git config user.email git文件的三种状态 对于任何一个文件，在 Git 内都只有三种状态： 已提交（committed） 已修改（modified） 已暂存（staged） 已提交表示该文件已经被安全地保存在本地数据库中了； 已修改表示修改了某个文件，但还没有提交保存； 已暂存表示把已修改的文件放在下次提交时要保存的清单中。 由此我们看到 Git 管理项目时，文件流转的三个工作区域：Git 的工作目录，暂存区域，以及本地仓库。 每个项目都有一个 Git 目录（译注：如果 git clone 出来的话，就是其中 .git 的目录；如果 git clone --bare 的话，新建的目录本身就是 Git 目录。），它是 Git 用来保存元数据和对象数据库的地方。该目录非常重要，每次克隆镜像仓库的时候，实际拷贝的就是这个目录里面的数据。 从项目中取出某个版本的所有文件和目录，用以开始后续工作的叫做工作目录。这些文件实际上都是从 Git 目录中的压缩对象数据库中提取出来的，接下来就可以在工作目录中对这些文件进行编辑。 所谓的暂存区域只不过是个简单的文件，一般都放在 Git 目录中。有时候人们会把这个文件叫做索引文件，不过标准说法还是叫暂存区域。 基本的 Git 工作流程如下： 在工作目录中修改某些文件。 对修改后的文件进行快照，然后保存到暂存区域。add 提交更新，将保存在暂存区域的文件快照永久转储到 Git 目录中。commit 所以，我们可以从文件所处的位置来判断状态：如果是 Git 目录中保存着的特定版本文件，就属于已提交状态；如果作了修改并已放入暂存区域，就属于已暂存状态；如果自上次取出后，作了修改但还没有放到暂存区域，就是已修改状态。 放弃修改 工作区的修改，还未add到暂存区。 单个文件/文件夹： git checkout – filename 所有文件/文件夹： git checkout . git checkout其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原”。 本地修改/新增了一堆文件，已经git add到暂存区 单个文件/文件夹： git reset HEAD filename 所有文件/文件夹： git reset HEAD . 本地通过git add &amp; git commit 之后，想要撤销此次commit git reset commit_id 这个id是你想要回到的那个节点，可以通过git log查看，可以只选前6位 // 撤销之后，你所做的已经commit的修改还在工作区！ git reset --hard commit_id 这个id是你想要回到的那个节点，可以通过git log查看，可以只选前6位 // 撤销之后，你所做的已经commit的修改将会清除，仍在工作区/暂存区的代码也将会清除！ git add &amp; git commit &amp; git push之后，想撤销push。 按照第三步撤销以后，下次push的时候在命令最后加上 --force 移除文件 要从 Git 中移除某个文件，就必须要从已跟踪文件清单中移除（确切地说，是从暂存区域移除），然后提交。可以用 git rm 命令完成此项工作，并连带从工作目录中删除指定的文件，这样以后就不会出现在未跟踪文件清单中了。 12git rm filename # 取消跟踪且删除文件git rm --cached filename # 仅仅取消跟踪 比较文件 git diff filename:比较工作区和暂存区 git diff HEAD – filename:比较工作区和版本库的最新版本 忽略文件 一般我们总会有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。 通常都是些自动生成的文件，比如日志文件，或者编译过程中创建的临时文件等。 在这种情况下，我们可以创建一个名为 .gitignore 的文件，列出要忽略的文件模式。 来看一个实际的例子： 123$ cat .gitignore*.[oa]*~ 第一行告诉 Git 忽略所有以 .o 或 .a 结尾的文件。一般这类对象文件和存档文件都是编译过程中出现的。 第二行告诉 Git 忽略所有以波浪符（~）结尾的文件。 此外，你可能还需要忽略 log，tmp 或者 pid 目录，以及自动生成的文档等等。 要养成一开始就设置好 .gitignore 文件的习惯，以免将来误提交这类无用的文件。 文件 .gitignore 的格式规范如下： 所有空行或者以 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式可以以（/）开头防止递归。 匹配模式可以以（/）结尾指定目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。 所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。 星号（*）匹配零个或多个任意字符；[abc] 匹配任何一个列在方括号中的字符（这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c）；问号（?）只匹配一个任意字符；如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如 [0-9] 表示匹配所有 0 到 9 的数字）。 使用两个星号（*) 表示匹配任意中间目录，比如 a/**/z 可以匹配 a/z , a/b/z 或 a/b/c/z 等。 我们再看一个 .gitignore 文件的例子： 123456789101112131415# 忽略所有的.a文件，除了lib.a以外，这里顺序很重要，后面的如果跟前面的冲突，会覆盖前面的。*.a!lib.a# only ignore the TODO file in the current directory, not subdir/TODO/TODO# ignore all files in the build/ directory,include subdir/buildbuild/# ignore doc/notes.txt, but not doc/server/arch.txtdoc/*.txt# ignore all .pdf files in the doc/ directorydoc/**/*.pdf Git 分支 创建分支 git branch branchName 切换分支 git checkout branchName 删除分支 git branch -d branchName 合并分支（先切换到合并目的分支） git merge branchName 查看分支 git branch 查看哪些分支已经合并到当前分支 git branch --merged 删除远程分支 1、先查看远程分支 git branch -ｒ 2、使用下面两条命令来删除远程分支 git branch -r -d origin/branch-name git push origin :branch-name # 远程删除 git push origin --delete branch-name ##上一条不行的话就试试这一条 GitHub上下载单个目录或者文件 将源地址中的 /tree/master/ 改成 /trunk/ 在终端运行 svn checkout 修改后的地址 如果不是master分支而是其他名字分支的话，将 /trunk/ 换成 /branches/{branchname}/ 就可以了。]]></content>
      <categories>
        <category>git教程</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度可分离卷积]]></title>
    <url>%2F2019%2F10%2F22%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%B3%BB%E5%88%97%2F%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF%2F</url>
    <content type="text"><![CDATA[前言 深度可分离卷积（depthwise separable convolutions），这是 Xception 以及 MobileNet 系列的精华所在。而它最早是由Google Brain的一名实习生Laurent Sifre于2013年提出。 普通卷积 举一个具体例子，假设输入层的大小是 7×7×3（高×宽×通道），而过滤器的大小是 3×3×3。经过与一个过滤器的 2D 卷积之后，输出层的大小是 5×5×1（仅有一个通道）。 一般来说，两个神经网络层之间会应用多个过滤器。假设我们这里有 128 个过滤器。在应用了这 128 个 2D 卷积之后，我们有 128 个 5×5×1 的输出映射图（map）。然后我们将这些映射图堆叠成大小为 5×5×128 的单层。通过这种操作，我们可将输入层（7×7×3）转换成输出层（5×5×128）。空间维度（即高度和宽度）会变小，而深度会增大。 深度可分离卷积 现在使用深度可分卷积，看看我们如何实现同样的变换。 首先，我们将深度卷积应用于输入层。但我们不使用 2D 卷积中大小为 3×3×3 的单个过滤器，而是分开使用 3 个核。每个过滤器的大小为 3×3×1。每个核与输入层的一个通道卷积（仅一个通道，而非所有通道！）。每个这样的卷积都能提供大小为 5×5×1 的映射图。然后我们将这些映射图堆叠在一起，创建一个 5×5×3 的图像。经过这个操作之后，我们得到大小为 5×5×3 的输出。现在我们可以降低空间维度了，但深度还是和之前一样。 深度可分卷积——第一步：我们不使用 2D 卷积中大小为 3×3×3 的单个过滤器，而是分开使用 3 个核。每个过滤器的大小为 3×3×1。每个核与输入层的一个通道卷积（仅一个通道，而非所有通道！）。每个这样的卷积都能提供大小为 5×5×1 的映射图。然后我们将这些映射图堆叠在一起，创建一个 5×5×3 的图像。经过这个操作之后，我们得到大小为 5×5×3 的输出。 在深度可分卷积的第二步，为了扩展深度，我们应用一个核大小为 1×1×3 的 1×1 卷积。将 5×5×3 的输入图像与每个 1×1×3 的核卷积，可得到大小为 5×5×1 的映射图。 因此，在应用了 128 个 1×1x3的卷积之后，我们得到大小为 5×5×128 的层。 通过这两个步骤，深度可分卷积也会将输入层（7×7×3）变换到输出层（5×5×128）。 下图展示了深度可分卷积的整个过程。 深度可分离卷积的优缺点 所以，深度可分卷积有何优势呢？效率！相比于 2D 卷积，深度可分卷积所需的操作要少得多。 普通卷积下，有 128 个 3×3×3 个核移动了 5×5 次，也就是 128 * 3 * 3 * 3 * 5 * 5 = 86400 次乘法。 可分离卷积在第一个深度卷积步骤，有 3 个 3×3×1 核移动 5×5 次，也就是 33315*5 = 675 次乘法。在 1x1x3 卷积的第二步，有 128 个 1×1×3 核移动 5×5 次，即 128 * 1 * 1 * 3 * 5 * 5 = 9600 次乘法。因此，深度可分卷积共有 675 + 9600 = 10275 次乘法。这样的成本大概仅有 2D 卷积的 12%！ 所以，对于任意尺寸的图像，如果我们应用深度可分卷积，我们可以节省多少时间？让我们泛化以上例子。现在，对于大小为 H×W×D 的输入图像，如果使用 Nc 个大小为 hhD 的核执行 2D 卷积（步幅为 1，填充为 0，其中 h 是偶数）。为了将输入层（HWD）变换到输出层（(H-h+1)* (W-h+1) * Nc），所需的总乘法次数为： Nc * h * h * D * (H-h+1) * (W-h+1) 另一方面，对于同样的变换，深度可分卷积所需的乘法次数为： D * h * h * 1 * (H-h+1) * (W-h+1) + Nc 1 * 1 * D * (H-h+1) * (W-h+1) = (h * h + Nc) * D * (H-h+1) * (W-h+1) 则深度可分卷积与 2D 卷积所需的乘法次数比为： 1Nc+1h2\frac{1}{_{Nc}}+\frac{1}{h^{2}}Nc​1​+h21​ 现代大多数架构的输出层通常都有很多通道，可达数百甚至上千。对于这样的层（Nc &gt;&gt; h），则上式可约简为 1 / h²。基于此，如果使用 3×3 过滤器，则 2D 卷积所需的乘法次数是深度可分卷积的 9 倍。如果使用 5×5 过滤器，则 2D 卷积所需的乘法次数是深度可分卷积的 25 倍。 使用深度可分卷积有什么坏处吗？当然是有的。深度可分卷积会降低卷积中参数的数量。因此，对于较小的模型而言，如果用深度可分卷积替代 2D 卷积，模型的能力可能会显著下降。因此，得到的模型可能是次优的。但是，如果使用得当，深度可分卷积能在不降低你的模型性能的前提下帮助你实现效率提升。 depthwise separable convolutions实战 以下是dw卷积块的Pytorch代码实现： 123456789101112import torch.nn as nndef conv_dw(inp, oup, stride): return nn.Sequential( nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False), nn.BatchNorm2d(inp), nn.ReLU(inplace=True), nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), nn.ReLU(inplace=True), ) 注意：第一步里的卷积里有个参数groups=inp，这就是每个通道单独进行卷积。 conv_dw在Mobilenet里的应用： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import torch.nn as nnimport torch.nn.functional as Fclass Mb(nn.Module): def __init__(self, num_classes=1000): super(Mb, self).__init__() def conv_bn(inp, oup, stride): return nn.Sequential( nn.Conv2d(inp, oup, 3, stride, 1, bias=False), nn.BatchNorm2d(oup), nn.ReLU(inplace=True) ) def conv_dw(inp, oup, stride): return nn.Sequential( nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False), nn.BatchNorm2d(inp), nn.ReLU(inplace=True), nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), nn.ReLU(inplace=True), ) self.model = nn.Sequential( conv_bn( 3, 32, 2), conv_dw( 32, 64, 1), conv_dw( 64, 128, 2), conv_dw(128, 128, 1), conv_dw(128, 256, 2), conv_dw(256, 256, 1), conv_dw(256, 512, 2), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 1024, 2), conv_dw(1024, 1024, 1), nn.AvgPool2d(7), ) self.fc = nn.Linear(1024, num_classes) def forward(self, x): x = self.model(x) x = F.avg_pool2d(x, 7) x = x.view(-1, 1024) x = self.fc(x) return x 参考资料： 深度可分离卷积（Xception 与 MobileNet 的点滴） 一文读懂 12种卷积方法]]></content>
      <categories>
        <category>深度学习基石系列</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>基石</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python类方法与静态方法]]></title>
    <url>%2F2019%2F10%2F15%2FPython%E5%9F%BA%E7%A1%80%2FPython%E7%B1%BB%E6%96%B9%E6%B3%95%E4%B8%8E%E9%9D%99%E6%80%81%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[classmethod和staticmethod 在遇到的项目代码中，在类里面经常看到这两个装饰器。今天就来具体说说这这两货是干嘛的。 一般情况下，我们定义一个类，里面会有一些属性和方法。我们通过类实例化一个对象，这个对象就有了这些属性和方法，就能正常调用了。 Python中的类也是一个普通对象，如果需要直接使用这个类，例如将类作为参数传递到其他函数中，又希望在实例化这个类之前就能提供某些功能。这个时候就需要用到classmethod和staticmethod了。 这两者的区别在于在存在类的继承的情况下对多态的支持不同。 用法 看以下的一个简单例子： 12345678910111213141516class A(object): def m1(self, n): print("self:", self) ## self相当于实例化对象本身 @classmethod def m2(cls, n): print("cls:", cls) ## cls相当于类对象本身 @staticmethod def m3(n): passa = A()a.m1(1) A.m2(1) A.m3(1) 什么时候使用classmethod？什么时候用staticmethod？ 查了很多资料，没看到特别好的说明。可能也只有在多写项目代码后才能体会二者的区别。这里就我个人理解做一点简要说明。 有的时候再实例化之前，就需要先和类做一定的交互。这个时候就需要classmethod或者staticmethod。 如果只是不需要访问类中任何实例方法和属性，仅仅作为一个函数实现某种功能，这种情况下适合使用staticmethod。 其他情况，适合使用classmethod。 再简单一点来说，静态方法就是个一般的函数，完全可以放在类的外面，只不过面象对象一般都是通过类来实现，就把这个函数给封到类里面去了。 欢迎补充 关于classmethod和staticmethod的更多细节，欢迎补充。]]></content>
      <categories>
        <category>Python学习笔记</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python多线程与多进程]]></title>
    <url>%2F2019%2F10%2F15%2FPython%E5%9F%BA%E7%A1%80%2FPython%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"></content>
      <categories>
        <category>Python学习笔记</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[杂谈之70建国周年]]></title>
    <url>%2F2019%2F10%2F01%2F%E6%9D%82%E6%84%9F%2F%E6%9D%82%E8%B0%88%E4%B9%8B70%E5%BB%BA%E5%9B%BD%E5%91%A8%E5%B9%B4%2F</url>
    <content type="text"><![CDATA[70周年 今天的朋友圈格外的热闹，各种段子各种梗玩的不亦乐乎，我们从未如此自豪。 以前，我们只有解放军陆军可以拿来吹嘘一波，自豪一番。现如今，海、空、火箭军一年一个样，舰船饺子搬的下海，20系列飞机也基本形成了战斗力、东风快递也频频亮相。 国庆阅兵中亮相的20系列飞机和东风家族大杀器自不必说，今年格外引人注意的还有我们的女兵。那气质、那笑容，一笑倾国。 20年前 1999年5月，美国最先进的一架B-2A隐形轰炸机经过15小时的长途飞行，在南联盟上空，投下5枚精确制导重型炸弹，轰炸了中国驻南斯拉夫联盟大使馆。 消息传来，群情激愤！然而除了群情激愤，除了外交谴责，别无他法！ 这一炸，炸掉了中国民间对美国的全部好感。在此之前，政府一直宣传“帝国主义亡我之心不死”，但是民间对此却没什么反应。很多人都已经深信美国是自由的灯塔，相信美国民主的那套东西。 这一炸也炸醒了中国政府，世界永远是那个世界，落后就要挨打。这是我们第二次学到这一点。耻辱而深刻。 995工程 1999年5月美国轰炸中国驻南联盟大使馆后，我们对当时的国防建设和国民经济建设的关系进行了调整，以前是国防建设为经济建设让路，当时改为提出协调发展，同步发展。 在这样一个巨大的刺激之下，中国国防工业部门开会制定了命名为“995”军备发展规划。 今天让我们引以为傲的“东风21D、东风26、31、41”系列战略导弹、“歼20”隐身战机、“001A”型国产航母等都是995工程的产物。 2003年 2003年3月，美国绕开联合国直接武装进攻伊拉克。 战争之前，我国当时的预测是美国会陷入与伊拉克的人民战争之中，陷入持久战。结果美军25天就全歼了伊拉克军队。中国震惊，吓出一身冷汗。 这场战争让我们见识到了什么是21世纪的战争。美军在这场战争中使用了几乎所有最先进武器装备系统,包括侦察、预警、导航、定位等强大的卫星天基系统; 大功率电磁干扰、网络干扰和无线电干扰侦察系统; 隐形战斗轰炸机等隐形战略攻击武器; 远射程、抗干扰的精确制导武器等。美军在天、空、海、磁等领域的绝对控制和远程精确制导打击优势得到充分的体现。 不得不说，美国是真的牛逼，也是一个很好的老师。我们与美国的差距要比想象中的更大，也促进我们向信息化作战和协同作战发展。 今天的阅兵中仅仅展示了武器，其实更多的东西没法展示。但是我们却一直在强调信息化作战以及恶劣环境下作战，这一点，军方比我们每个普通人都要更加清醒。 2018美国制裁中兴 18年，美国一纸禁令，一个小小的芯片搞残了半个中兴。中国再次被打脸。 落后就要挨打，这一次得刻到骨子里去了。国家再一次加大力度于芯片产业。 之后的华为事件、香港之乱也都扯掉了美国最后的几块遮羞布。当他打得过你的时候他会很优雅的制裁你，当他弄不过你的时候就开始各种下三滥的手段一起上了。从来如此。 尾记 不得不说，中国骨子里的天赋注定成为这个世界的超级大国。每一次的打击，只要打不死中国，必定越挫越勇，越打越壮。而能打败中国的，只能是从中国内部打败，外部势力绝无可能。 99年大使馆被炸，催生了995工程，20年后硕果累累。 近两年的贸易战也是如此，且看20年后的中国，到时候我们一定无比怀念今日的特朗普。]]></content>
      <categories>
        <category>个人杂感</category>
      </categories>
      <tags>
        <tag>杂感</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux tree命令]]></title>
    <url>%2F2019%2F09%2F25%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AALinux%E5%91%BD%E4%BB%A4%2FLinux%20tree%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[从需求说起 很多时候，比如写博文或者写README.md的时候，我们常常需要以树状图列出目录的内容用作说明。Linux有一个很强大的命令刚好可以实现这个需求。 但是除了简单的显示目录结构外，我们还有一些额外过分的需求，比如只显示某些目录不显示文件、排除某类型的文件、排除深度超过多少的文件等等。 没关系，tree都能实现！！！ 语法 tree [-aACdDfFgilLnNpqstux][-I &lt;范本样式&gt;][-P &lt;范本样式&gt;][目录…] 参数说明(以下为常用的几条，完整版见链接) -C 在文件和目录清单加上色彩，便于区分各种类型。 -d 只显示目录不显示文件。 -I &lt;范本样式&gt; 排除指定内容 -P &lt;范本样式&gt; 只显示符合范本样式的文件或目录名称。 -L 只显示指定层级的内容 示例 原样显示： 颜色显示： 排除某个目录： 组合多个参数使用：]]></content>
      <categories>
        <category>每天一个Linux命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alexnet基本思想与Pytorch实现]]></title>
    <url>%2F2019%2F09%2F17%2F%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2FAlexnet%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3%E4%B8%8EPytorch%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前言 说来惭愧，作为一名算法工程师，都没怎么好好读过论文。这几年下来，都是需要什么需求，比如毕设课题是关于目标检测方面的，然后先上github搜代码，再大致看看代码涉及到的论文，再把代码弄下来跑跑自己的数据，稍微修改点参数什么的。 What a shame！！！ 这样下去不行滴。正所谓基础不牢，地动山摇。正好最近鼓捣了一个个人博客，这次就系统性的恶补一下所有的基础知识。并将所有知识整理成个人笔记，也希望遇到志同道合的朋友一起学习学习。 接下来的内容分为两个基础系列，一个是基础骨干网络系列，从Alexnet到resnet，还包括mobilenet以及EfficientNet等；另一个是目标检测系列，包括RCNN全系列、YOLO全系列、SSD等。 所有的博客都将采取论文+代码的形式。 论文内容：解决了现有技术的哪些痛点问题、论文中的一些小技巧和细节。 代码内容：Pytorch、GTX1080Ti、数据集介绍、训练时间、单张图片预测时间、模型大小、参数量多少、浮点运算次数、准确率。 从Alexnet开始 Alexnet可谓是深度学习大狂潮的一个起飞点，他成功的踹开了深度学习的大门，具有里程碑式的意义。 2012年，Alex Krizhevsky、Ilya Sutskever在多伦多大学Geoff Hinton的实验室设计出了一个深层的卷积神经网络AlexNet，夺得了2012年ImageNet LSVRC的冠军，且准确率远超第二名（top5错误率为15.3%，第二名为26.2%），引起了很大的轰动。AlexNet可以说是具有历史意义的一个网络结构，在此之前，深度学习已经沉寂了很长时间，自2012年AlexNet诞生之后，后面的ImageNet冠军都是用卷积神经网络（CNN）来做的，并且层次越来越深，使得CNN成为在图像识别分类的核心算法模型，带来了深度学习的大爆发。 注：Alexnet的出现证明的深度学习的巨大潜力，但是并不能算是深度学习的开端。 深度学习的开端应该是卷积的应用。1989年，LeCun结合反向传播算法与权值共享的卷积神经层发明了卷积神经网络，并首次将卷积神经网络成功应用到美国邮局的手写字符识别系统中。1998年，LeCun提出了卷积神经网络的经典网络模型LeNet-5，再次提高手写字符识别的正确率。 在卷积应用之前，也是有神经网络的，不过那时候的神经网络只能算是一个多层感知机，是神经元全连接的形式。一旦加深网络的层数，参数量会剧增。 论文：imagenet-classification-with-deep-convolutional-neural-networks 现存的问题： 网络的层数比较少，如果增加网络的深度，会导致梯度弥散或者梯度消失，模型参数无法得到有效更新。 模型运算量大，训练时间长。 论文中的创新点： 使用了非线性激活函数：ReLU 防止过拟合的方法：Dropout，数据扩充（Data augmentation） 其他：多GPU实现，LRN归一化层的使用（注：LRN的有效性存在争议，不用的时候反而更好的情况也存在。） 个人感受：可以看到，Alexnet中的几条创新之处都不复杂，看起来都很简单，但是却实实在在的创造了奇迹，打开了一个新时代。 在Alexnet之前也是有卷积这个概念的，也有过成功的应用。但是那时候的卷积神经网络能做到的事情，别的方法也同样能做到，而且还能做的更好。 在这之前，神经网络的发展一度陷入瓶颈期，他存在很多问题，饱受人们质疑。 的确，指出一种理论、一种方法、一种现实中的不足和缺陷是很容易的，我们都能做到。很多人在看出了这么一点点现实的时候就开始有了莫名的优越感，甚至开始指点江山，有那么一种众人皆醉我独醒的感受。 但是啊，指出问题很容易，但是给出解决方案却不容易啊。有的人醉心于前者，有的人却埋头于后者。 好了，废话不多说，回到正题。Alexnet的几点创新之中，个人认为Relu和Dropout是最最关键的两点。 relu relu自然不必说了，没有他，网络根本深不起来。 Dropout Dropout常常被人们所忽略，但是他的重要性不容质疑。“神经网络之父”Hinton在后来很长一段时间里的演讲中都拿Dropout说事。 深度学习的网络表达能力实在太强，以至于他甚至能将每一条训练输入的所有特征都记住，从而很容易导致过拟合。也就是他能将训练数据中的每一条都预测准确，这样的模型在测试集中的表现就很糟糕了。 看上面这张图，绿色线就是过拟合。可以说是很直观了。将一些没用的东西也学到了。 Alexnet的Pytorch实现 代码环境： pytorch 1.2.0 python 3.7 ubuntu 18.04 GTX1080Ti 单GPU 一个好的项目一定要是逻辑清晰的，结构明晰而简单的。给人一种看上去就很赏心悦目的感觉，一定一定不要混乱不堪。 代码结构： 数据加载 模型构建 训练 验证 应用 github 内容 结果 数据集 Caltech256 训练时间 单张图片预测时间 模型大小 参数量多少 浮点运算次数 * 准确率 67% 未完待续]]></content>
      <categories>
        <category>论文学习笔记与代码实现</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
        <tag>Deep Learning</tag>
        <tag>Alexnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL安装使用]]></title>
    <url>%2F2019%2F09%2F10%2FMySQL%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[MySQL 1、安装 1、检测是否已安装 $ service mysql status 如果出现下面的结果则表示已安装 2、安装 如果上一步没有安装的话直接安装： $ sudo apt-get install mysql-server mysql-client #安装服务端与客户端 $ sudo apt-get install mysql-workbench #安装可视化工具 3、检测启动状态 $ service mysql status 以下两张图分别是启动和停止状态。 启动mysql： 方式一：sudo /etc/init.d/mysql start 方式二：sudo service mysql start 停止mysql： 方式一：sudo /etc/init.d/mysql stop 方式二：sudo service mysql stop 重启mysql： 方式一：sudo /etc/init.d/mysql restart 方式二：sudo service mysql restart 4、登录 初始情况下 root 账户没有密码，这时普通用户并不能直接使用 mysql 命令直接控制台登录，更不能使用 mysql-workbench 直接连接登录。主要是由于 mysql.user 这张表中 root 用户的 plugin 字段值为 auth_socket，改为 mysql_native_password 即可。同时为了方便之后使用，我们在接下来的操作中顺带给 root 账户设置密码。 123456789101112#查看所有用户select host,user from user;#删除用户名为yang，host为%的用户drop user yang\@&apos;%&apos;;#删除数据库drop database &lt;数据库名&gt;;#创建用户并赋予权限GRANT ALL ON *.* TO &apos;yang&apos;@&apos;localhost&apos; WITH GRANT OPTION;#创建数据库CREATE DATABASE file_base_info;#查看当前用户select USER(); 5、远程登录 第一、允许MySQL数据库被远程连接 编辑/etc/mysql/mysql.conf.d/mysqld.cnf 配置文件，注释bind-address = 127.0.0.1这一行，然后重启MySQL服务。 第二、授权允许远程连接的用户 以授权root用户为例 mysql&gt; grant all privileges on . to root@&quot;%&quot; identified by “password” with grant option;]]></content>
      <categories>
        <category>教程</category>
        <category>MySQL使用教程</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python参数解析工具]]></title>
    <url>%2F2019%2F09%2F09%2FPython%E5%9F%BA%E7%A1%80%2Fpython%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[absl vs argparse 在Python项目中，我们经常看到absl或者argparse的身影。其中，absl是谷歌出品，argparse为Python自带。 这两个都是十分简洁好用的参数解析工具。 先来个直观的感受吧。看看二者的用法。 absl的用法： 12345678910111213141516171819from absl import appfrom absl import flags FLAGS = flags.FLAGS # flags.DEFINE_string("name", None, "Your name.")flags.DEFINE_integer("num_times", 1, "Number of times to print greeting.") # 指定必须输入的参数flags.mark_flag_as_required("name") def main(argv): del argv # 无用 for i in range(0, FLAGS.num_times): print('Hello, %s!' % FLAGS.name) if __name__ == '__main__': app.run(main) # 和tf.app.run()类似 argparse的用法： 123456789101112131415161718192021import argparsedef parse_args(): parser = argparse.ArgumentParser(description='Train segmentation network') parser.add_argument('--cfg', help='experiment configure file name', required=True, type=str) parser.add_argument('opts', help="Modify config options using the command-line", default=None, nargs=argparse.REMAINDER) args = parser.parse_args() return argsdef main(): args = parse_args() print(args.cfg) absl详解 用过TensorFlow的肯定知道tf.app.flags的用法和这个absl非常相似。 没错，实质上他俩就是一个东西，同属谷歌出品，只是将其放入了TensorFlow当中。这个absl在很早之前就有了，是谷歌的c++参数解析库。后来很好用才有了python版本。 在之后的TensorFlow版本中，将很多臃肿的工具类库都移除了，tf.app.flags也移除了。所以之后想用的话只能安装absl了。 安装 pip install absl-py 参数类型 直接看源码： 123456789101112131415DEFINE_string = _defines.DEFINE_stringDEFINE_boolean = _defines.DEFINE_booleanDEFINE_bool = DEFINE_boolean # Match C++ API.DEFINE_float = _defines.DEFINE_floatDEFINE_integer = _defines.DEFINE_integerDEFINE_enum = _defines.DEFINE_enumDEFINE_enum_class = _defines.DEFINE_enum_classDEFINE_list = _defines.DEFINE_listDEFINE_spaceseplist = _defines.DEFINE_spaceseplistDEFINE_multi = _defines.DEFINE_multiDEFINE_multi_string = _defines.DEFINE_multi_stringDEFINE_multi_integer = _defines.DEFINE_multi_integerDEFINE_multi_float = _defines.DEFINE_multi_floatDEFINE_multi_enum = _defines.DEFINE_multi_enumDEFINE_multi_enum_class = _defines.DEFINE_multi_enum_class 常用的也就以下几种，记住就行了： DEFINE_string DEFINE_float DEFINE_integer 再往下就是： DEFINE_bool (同DEFINE_boolean) DEFINE_enum DEFINE_list 其他的不用管了，有需求的时候再查文档就好了。 定义 超级简单，如下。其中三个参数分别是名字、默认值和描述。 123from absl import flags flags.DEFINE_string("name", None, "Your name.") 必须指定的参数 flags.mark_flag_as_required(“name”) 参数使用 123FLAGS = flags.FLAGS # 使用的时候是大写的FLAGS，其他时候都是小写。print(FLAGS.name) argparse详解 Argparse的使用主要有三个步骤： 创建 ArgumentParser() 对象 调用 add_argument() 方法添加参数 使用 parse_args() 解析添加的参数 参数说明 add_argument 12ArgumentParser.add_argument(name or flags...[, action][, nargs][, const][, default][, type][, choices][, required][, help][, metavar][, dest]) 每个参数解释如下: name or flags - 选项字符串的名字或者列表，例如 foo 或者 -f, –foo。 action - 命令行遇到参数时的动作，默认值是 store。 store_const，表示赋值为const； append，将遇到的值存储成列表，也就是如果参数重复则会保存多个值; append_const，将参数规范中定义的一个值保存到一个列表； count，存储遇到的次数；此外，也可以继承 argparse.Action 自定义参数解析； nargs - 应该读取的命令行参数个数，可以是具体的数字，或者是?号，当不指定值时对于 Positional argument 使用 default，对于 Optional argument 使用 const；或者是 * 号，表示 0 或多个参数；或者是 + 号表示 1 或多个参数。 const - 一个在 action 和 nargs 选项所需的常量值。 default - 不指定参数时的默认值。 type - 命令行参数应该被转换成的类型。 choices - 参数可允许的值的一个容器。 required - 可选参数是否可以省略 (仅针对可选参数)。 help - 参数的帮助信息，当指定为 argparse.SUPPRESS 时表示不显示该参数的帮助信息. metavar - 在 usage 说明中的参数名称，对于必选参数默认就是参数名称（上面的 name or flags），对于可选参数默认是全大写的参数名称. dest - parse_args() 方法返回的对象所添加的属性的名称。默认情况下，对于可选参数选取最长的名称，中划线转换为下划线. 官方示例 123456789101112131415161718# 创建了 ArgumentParser 对象，该对象具有解析命令行转为 Python 数据类型的全部信息parser = argparse.ArgumentParser(description='Process some integers.')# 增加参数parser.add_argument('integers', metavar='N', type=int, nargs='+', help='an integer for the accumulator')parser.add_argument('--sum', dest='accumulate', action='store_const', const=sum, default=max, help='sum the integers (default: find the max)')parser.add_argument('--foo', action='store_true')parser.add_argument('--bar', action='store_false')# 调用 parse_args() 会返回对象的两个属性，integers 和 accumulate。 integers 属性是一个列表# 如果在命令行中指定了 --sum，例如 python a.py 1 --sum ，则 accumulate 属性将是 sum() 函数，# 如果没有加上 --sum，例如 python a.py 1，则 accumulate 为 max() 函数&gt;&gt;&gt; parser.parse_args(['--sum', '7', '-1', '42'])Namespace(accumulate=&lt;built-in function sum&gt;, integers=[7, -1, 42]) 这里的 store_const 可以这么理解，它对应的属性是可以手动赋值的，比如这里的 accumulate，该属性值是自动获取： 如果参数中使用了 --sum，那么后面不赋值，accumulate 也会根据取 const 指定的值取值； 如果参数中没有出现 --sum，那么这个可选参数会去 default 指定的值，如果不指定默认值，会取值 None 这里的 store_false 可以这么理解，它对应的属性也是不可以赋值的，是自动获取的： 命令行使用了 --foo，这个属性就为 True，否则为 False 命令行使用了 --bar，这个属性就为 False，否则就为 True 互斥示例 123group = parser.add_mutually_exclusive_group()group.add_argument("-v", "--verbose", action="store_true")group.add_argument("-q", "--quiet", action="store_true")]]></content>
      <categories>
        <category>Python学习笔记</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac 下 Hexo的安装]]></title>
    <url>%2F2019%2F09%2F07%2Fhexo%E6%95%99%E7%A8%8B%2FMac%20%E4%B8%8B%20Hexo%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Mac 下 Hexo的安装 准备条件 硬件环境： Mac 笔记本 软件环境： GitHub 账号 Git Node.js npm hexo 本地站点搭建 安装Git 在 Mac OS X 中自带 Git，不需要安装 Git，在 Mac terminal 中输入以下命令可以查看 git 版本号： 1git --version 安装Node.js Node.js 可以直接从官网下载，然后下一步下一步即可完成安装，如下图所示： 从上图中可以看出，node.js 安装完成以后，npm 也被安装完成，简化了工作。检测安装是否成功，可以在 Mac terminal 中输入以下命令查看 node.js 和 npm 的版本号： 12node -vnpm -v 安装Hexo Git、Node.js 和 npm 安装完成以后，接下来就可以安装 Hexo 了。在 Mac terminal 中输入以下命令即可执行安装： 1sudo npm install -g hexo-cli 这里开始就出现各种坑了。一直报错说是没有权限。 Error: EACCES: permission denied, access ‘/usr/local/lib/node_modules’ 我都已经给了sudo权限了依然如此。 接下来又放了两个权限大招： 加权限 root用户执行安装 加权限 12sudo chown -R `whoami` /usr/local/lib/node_modulessudo chmod -R 774 /usr/local/lib/node_modules/* root用户执行安装 1su #如果出现sorry的话，就用sudo su进去，再给root用户设置密码 passwd root 大招放尽，依然没有鸟用。安装时还是出现err 但是！！！ 仔细一看，报错的一直是fsevents这么个东西啊！！ hexo-cli@2.0.0 这句话就表明hexo已经安装好了啊！！！！！ 输入以下命令查看 Hexo 的版本号： 1hexo -v 如下图所示： 推荐另一种安装方法 安装nvm curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.34.0/install.sh | sh 安装后检查一下是否安装成功： command -v nvm 如果输出nvm三个字则表示安装成功，在bash shell下使用上述命令。 安装npm，node nvm install stable 检查安装: node -v npm -v 安装hexo npm install -g hexo-cli 检查安装: hexo -v 搞定！！！！ 初始化 Hexo 1234hexo init blogcd blognpm installhexo server 在本地查看效果 打开浏览器，输入 http://localhost:4000/ 部署到 GitHub 以上操作的结果是可以生成一个在本地浏览的博客站点，若想部署到 GitHub 上，请按照如下方式操作即可。因为本博客是托管在 GitHub 上的，所以一个 GitHub 账号是必不可少的，在 GitHub 注册账号这里就不叙述了。 创建 repository，Repository name 的格式必须是：yourGitHubId.github.io，例如我的是：whuhit.github.io Description 可以为空； 免费服务的话，只能选择 Public (公开的) 安装 Git 部署器 npm install hexo-deployer-git --save 修改配置文件 在 hexo 的根目录下会有 _config.yml 文件，打开 _config.yml 文件，在文件最末尾，修改如下配置： 123456#Deployment##Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: https://github.com/whuhit/whuhit.github.io branch: master 部署 在 hexo 目录下执行以下命令， 即可完成对将静态博客部署到 GitHub 上 hexo deploy 部署成功以后，在浏览器中输入 http://whuhit.github.io 即可在线浏览自己的博客啦~ 创建新博客 执行以下命令： hexo new “test_blog” 在 /hexo/source/_posts/ 目录下即可看到一个名为 test-blog.md 文件。 发布新博客 编辑好博客以后，即可执行以下命令部署到GitHub上： 123hexo cleanhexo generatehexo deploy PS：一些快捷键： 1234hexo g == hexo generatehexo d == hexo deployhexo s == hexo serverhexo n == hexo new]]></content>
      <categories>
        <category>教程</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac开发必备工具]]></title>
    <url>%2F2019%2F09%2F07%2FMac%E5%BC%80%E5%8F%91%E5%BF%85%E5%A4%87%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[1、Homebrew 安装与配置 Homebrew 的安装非常简单，将下面这条命令粘贴到终端： 1/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 等待命令执行完毕。其他配置见官网中文说明。 常用命令 brew help 查看帮助 brew install 安装软件包 brew uninstall 卸载软件包 brew list [–versions] 列出已安装的软件包(包括版本) brew search 查找软件包 brew info 查看软件包信息 brew update 更新brew brew outdated 列出过时的软件包（已安装但不是最新版本） brew upgrade [] 更新过时的软件包（不指定软件包表示更新全部） brew doctor 检查brew运行状态 2、iTerm 2 安装 使用 Homebrew 命令brew install iTerm2安装。 配置 关于 iTerm 2 的配置可以参考官网介绍，然后根据自己的喜好配置。这里不多说，参考网上各大佬的配置就好。 3、fish fish 可以根据输入自动匹配历史命令。这一点zsh也可以实现，不过需要自己安装插件。 它的一大特点是开箱即用，没有zsh那些繁琐的配置。官网 安装 fish从安装到使用都十分的简单,安装只需要下面一条命令就可以了。 brew install fish 安装完了之后，在/etc/shells文件后面加上一句： /usr/local/bin/fish 配置 将用户默认命令由 shell 切换到 fish chsh -s /usr/local/bin/fish 如果 /usr/local/bin/fish 不在 /etc/shells 中 则会遇到 chsh: /usr/local/bin/fish: non-standard shell 的错误 其他配置 1、默认使用anaconda环境下Python 在~/.config/fish/config.fish文件下（没有的话一个创建）加入以下内容： set PATH /Users/yang/anaconda3/anaconda3/bin $PATH 换成你自己的路径就行。 2、自定义欢迎语 在~/.config/fish/config.fish文件下加入以下内容： set fish_greeting ‘Talk is cheap. Show me the code.’ 3、自定义函数 fish还可以自定义一些简单的功能。 比如一般我们pip install的时候如果速度比较慢，则可以安装文件的时候加一句 -i https://pypi.tuna.tsinghua.edu.cn/simple 就可以临时换源了。 pip install opencv_python -i https://pypi.tuna.tsinghua.edu.cn/simple 但是每次都要去查这个网址，很麻烦，所以我们直接定义一个fast_pip，内容如下，加入~/.config/fish/config.fish文件中就行了。 123function fast_pip pip --no-cache-dir install $argv[1] -i https://pypi.tuna.tsinghua.edu.cn/simpleend 之后想要临时换源安装的时候用以下命令就可以了： fast_pip opencv_python 4、QuickLook系列之预览图片 不知道从哪个版本开始，mac查看图片时，已经没法查看图片分辨率大小了。垃圾苹果。 后来，查了一堆资料，终于找到一个好东西：qlImageSize 安装方法： brew cask install qlimagesize 再次预览图片（空格键）的时候就会显示分辨率了。如果安装完没显示的话重启mac就行了。]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>效率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python之pathlib包的使用]]></title>
    <url>%2F2019%2F09%2F06%2FPython%E5%9F%BA%E7%A1%80%2Fpython%E4%B9%8Bpathlib%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[pathlib简介 pathlib库在python 3.4以后已经成为标准库，基本上可以代替os.path来处理路径。它采用完全面对对象的编程方式。 在过去，文件的路径是纯字符串，现在它是一个pathlib.Path对象。 为什么要用pathlib 在Mac、Linux和Windows这三种不同的操作系统，其文件连接所用正反斜线是不一样的。 来看下三种不同的文件拼接方式： 1、直接字符串相加 1234567data_folder = "source_data/text_files/"file_to_open = data_folder + "raw_data.txt"f = open(file_to_open)print(f.read()) 点评：Low，非常的Low。而且并不能保证每个操作系统都能顺利执行。 p.s:我就是这么用的。（🥶） 2、使用os模块 123456789import os.pathdata_folder = os.path.join(&quot;source_data&quot;, &quot;text_files&quot;)file_to_open = os.path.join(data_folder, &quot;raw_data.txt&quot;)f = open(file_to_open)print(f.read()) 点评：没毛病。大多数人也是这么用的。但是反复使用os.path.join很啰嗦，是一点都不优雅！ 3、pathlib模块 1234567from pathlib import Pathdata_folder = Path(&quot;source_data/text_files/&quot;)file_to_open = data_folder / &quot;raw_data.txt&quot;print(file_to_open.read_text()) 看到没，是不是超级简单。 上面的data_folder和file_to_open都是一个Path对象，不再是一个字符串。 而这个对象有超多非常好用的方法和属性，也重载了一些操作符。 凡是os.path能做到的事情，pathlib.Path也一样能做到。而且能更优雅的实现os.path的功能。 再看几个更神奇的地方： 1234567891011121314151617from pathlib import Pathdata_folder = Path(&quot;source_data/text_files/&quot;)file_to_open = data_folder / &quot;raw_data.txt&quot;print(filename.name) #全名# &quot;raw_data.txt&quot;print(filename.suffix) #格式# &quot;.txt&quot;print(filename.stem) #去掉格式后的名字# &quot;raw_data&quot;print(filename.parent) #所在文件夹# source_data/text_files 之前一般要么是使用split不断的切分字符串，要么就是os.path一层一层的往上找父目录才能实现这些功能。 而pathlib一个属性全搞定。 和 os 功能对应的方法列表 os and os.path pathlib os.path.abspath Path.resolve os.chmod Path.chmod os.mkdir Path.mkdir os.rename Path.rename os.replace Path.replace os.rmdir Path.rmdir os.remove, os.unlink Path.unlink os.getcwd Path.cwd os.path.exists Path.exists os.path.expanduser Path.expanduser and Path.home os.path.isdir Path.is_dir os.path.isfile Path.is_file os.path.islink Path.is_symlink os.stat Path.stat, Path.owner, Path.group os.path.isabs PurePath.is_absolute os.path.join PurePath.joinpath os.path.basename PurePath.name os.path.dirname PurePath.parent os.path.samefile Path.samefile os.path.splitext PurePath.suffix 其他的一些功能 pathlib功能太多了，我这里就不一一列举了。 精力有限，我也不打算探索pathlib的全部功能，下面从我写代码时的一些现实需求出发，简单列一下几个常用的方法： 1、获取当前路径 1pwd = Path.cwd() 2、获取当前路径下的所有文件夹 12345678910pwd1 = Path.cwd()pwd2 = Path(&quot;.&quot;)x1 = [x for x in pwd1.iterdir() if x.is_dir()]x2 = [x for x in pwd2.iterdir() if x.is_dir()]# 这两者是有区别的，一个是绝对路径，一个是相对路径 print(pwd1 is pwd2)# false 3、获取当前路径下的所有文件 12pwd = Path(&quot;.&quot;)x1 = [x for x in pwd.iterdir() if x.is_file() and x.suffix == &quot;.py&quot;] 4、获取当前路径下的所有文件夹（包括子文件夹） 12pwd2 = Path(&quot;.&quot;)x2 = [x for x in pwd2.glob(&apos;**/*&apos;) if x.is_dir()] 5、获取当前路径下所有文件（包括子文件夹） 123pwd2 = Path(&quot;.&quot;)x2 = [x for x in pwd2.glob(&apos;**/*&apos;) if x.is_file()]x2 = [x for x in pwd2.glob(&apos;**/*.py&apos;) if x.is_file()] 6、拼接路径 12345data_folder = Path(&quot;source_data/&quot;)train_img = data_folder / &quot;train&quot; / &quot;image&quot;print(train_img) 7、父路径 123456789p = Path(&quot;source_data/train/image/people&quot;)print(p.parent)# source_data/train/imageprint(p.parent.parent)# source_data/trainprint(p.parent.parent.parent)# source_data 更多参考资料 你应该使用pathlib替代os.path pathlib路径库使用详解 python中文文档]]></content>
      <categories>
        <category>Python学习笔记</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下删除安装的pkg]]></title>
    <url>%2F2019%2F09%2F05%2FMac%E4%B8%8B%E5%88%A0%E9%99%A4%E5%AE%89%E8%A3%85%E7%9A%84pkg%2F</url>
    <content type="text"><![CDATA[Mac下删除安装的pkg 1、找到安装的pkg包的名称 比如我要卸载之前安装的Node.js 1pkgutil --pkgs | grep -i nodejs 如下图所示，找到两个包。之前安装的时候也是两个包，没毛病。 2、找到pkg包所在的文件位置 12pkgutil --files org.nodejs.npm.pkgpkgutil --files org.nodejs.node.pkg 3、删除相关文件 可以直接进入相关文件夹直接删除，另外也可以使用终端命令删除。 12cd /sudo pkgutil --files org.nodejs.node.pkg | xargs rm -r !!!千万不要用上面的命令，说多了都是泪，😔哎！ 4、忘记关联 12sudo pkgutil --forget org.nodejs.npm.pkgsudo pkgutil --forget org.nodejs.node.pkg]]></content>
      <categories>
        <category>安装卸载那些事</category>
      </categories>
      <tags>
        <tag>pkg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python多日志文件输出]]></title>
    <url>%2F2019%2F09%2F05%2FPython%E5%9F%BA%E7%A1%80%2Fpython%E5%A4%9A%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6%E8%BE%93%E5%87%BA%2F</url>
    <content type="text"><![CDATA[需求： 在实际写代码的过程中，我发现了这么一个问题： 在Python写文件的时候，如果一个文件需要写入几千行甚至上万行的时候，经常会漏写一些数据。这是不能容忍的。 这个时候我的解决办法一般就是通过Python的logging模块来往文件中写数据，这个时候没出现过漏写的情况。 但是，有的时候，我需要把不同的数据写入不同的文件。这个时候就需要设置一下才能实现了。 废话不多说，show you the code。如下： 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# 多日志文件的简单实现import pathlibimport loggingimport uuidfrom logging.handlers import TimedRotatingFileHandlerimport timeclass Plog(object): """docstring for Plog 级别 何时使用 DEBUG 详细信息，典型地调试问题时会感兴趣。 INFO 证明事情按预期工作。 WARNING 表明发生了一些意外，或者不久的将来会发生问题（如‘磁盘满了’）。软件还是在正常工作。 ERROR 由于更严重的问题，软件已不能执行一些功能了。 CRITICAL 严重错误，表明软件已不能继续运行了。 """ formatter = logging.Formatter(fmt='[%(asctime)s.%(msecs)03d] [%(levelname)08s] [%(lineno)03s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S') formatter2 = logging.Formatter('%(message)s') def __init__(self, log_file, level=logging.DEBUG,stream=True,msgOnly=False): pdir = pathlib.Path(log_file).parent if not pdir.exists(): pathlib.Path.mkdir(pdir,parents=True) # 父文件夹不存在则自动创建。 self.log_file = log_file self.level = level self.stream = stream self.log_name = str(uuid.uuid1()) # 区分不同日志。 self.logger = logging.getLogger(self.log_name) self.logger.setLevel(self.level) # 日志文件 # handler = logging.FileHandler(self.log_file, mode='a') # handler.setFormatter(Plog.formatter) # self.logger.addHandler(handler) handler = TimedRotatingFileHandler(self.log_file, when='D', encoding="utf-8") if msgOnly: handler.setFormatter(Plog.formatter2) else: handler.setFormatter(Plog.formatter) self.logger.addHandler(handler) # 终端流 if self.stream: streamHandler = logging.StreamHandler() streamHandler.setFormatter(Plog.formatter2) self.logger.addHandler(streamHandler) self.logger.debug("==========*****start to log*****==========") def __getattr__(self,item): return getattr(self.logger,item) # logging.getLogger(self.log_name) ## 本想在程序结束的时候添加一句日志结束标记 ## 但是一直报一个错 ## NameError: name 'open' is not defined ## 无法解决。所以只能退而求其次在程序开始的时候加开始标记了。 # def __del__(self): # logging.getLogger(self.log_name).debug("eixt code 00") # print("jisdjia")if __name__ == "__main__": a = Plog("a/a/a/a.log") # b = Plog("b.log")() for i in range(1000): a.debug("critical") time.sleep(1) # b.debug("debug") 不想这么麻烦建个类的话可以简单点，如下： 12345678910111213141516171819import logging#日志1logger1 = logging.getLogger("logger1")handler1 = logging.FileHandler("logger1.log", mode='a')logger1.addHandler(handler1)logger1.setLevel(logging.DEBUG)#日志2logger2 = logging.getLogger("logger2")handler2 = logging.FileHandler("logger2.log", mode='a')logger2.addHandler(handler2)logger2.setLevel(logging.INFO)print(logging.getLogger('logger1') is logger1) #truelogger1.debug("this is a debug msg")logger2.error("this is an error msg") 下面是关于logging模块的更多详细资料： Python日志库logging总结]]></content>
      <categories>
        <category>Python学习笔记</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为已绑定的自定义域名启用HTTPS访问]]></title>
    <url>%2F2019%2F09%2F04%2Fhexo%E6%95%99%E7%A8%8B%2Fhttps-setting%2F</url>
    <content type="text"><![CDATA[为已绑定的自定义域名启用HTTPS访问 打开 DNS 服务商的 DNS 记录修改页面。(我用的是阿里云的DNS) 原来域名 A 记录指向的IP地址是 192.30.252.153 和 192.30.252.154 ，增加下面几个新地址: 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 保存修改后，等待 DNS 记录生效。 然后，打开 GitHub Pages 设置页面： 1、找到处于不可选状态的Enforce HTTPS选项，如果不添加上面几条新的ip地址，旁边会提示: Unavailable for your site because your domain is not properly configured to support HTTPS。 2、将填在Custom domain里的自定义域名清空，保存，然后重新填上自定义域名，再保存。 3、现在可以勾选Enforce HTTPS选项了，这时会提示正在签发证书: Not yet available for your site because the certificate has not finished being issued。 4、证书签发成功后，可以使用 https 链接访问自定义域名了。 参考链接: Troubleshooting custom domains]]></content>
      <categories>
        <category>教程</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch奇技淫巧]]></title>
    <url>%2F2019%2F09%2F03%2FPytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Fpytorch%E5%A5%87%E6%8A%80%E6%B7%AB%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[pytorch奇技淫巧 1、cudnn加速 12torch.backends.cudnn.enabled = Truetorch.backends.cudnn.benchmark = True 设置 torch.backends.cudnn.benchmark=True 将会让程序在开始时花费一点额外时间，为整个网络的每个卷积层搜索最适合它的卷积实现算法，进而实现网络的加速。适用场景是网络结构固定（不是动态变化的），网络的输入形状（包括 batch size，图片大小，输入的通道）是不变的，其实也就是一般情况下都比较适用。反之，如果卷积层的设置一直变化，将会导致程序不停地做优化，反而会耗费更多的时间。 那么问题来了，我们需要记住在什么情况下才将这玩意儿设成true呢？ 我的答案是不需要，记那么多东西干啥？？？那不记住咱办？ 试！！！自己训练模型的时候将这个参数设为false和true，分别体验一下，体验几次后自然就知道了。 2、pytorch如何能够保证模型的可重复性 问题背景是这样的： 同样的模型，同样的参数，在GPU上运行的结果和CPU运行的结果有些不一致。 解决办法：在主函数中加上一句 1torch.backends.cudnn.deterministic = True 用以保证实验的可重复性。]]></content>
      <categories>
        <category>Pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 个性化配置]]></title>
    <url>%2F2019%2F09%2F03%2Fhexo%E6%95%99%E7%A8%8B%2Fhexo%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1、主题选择 之前看到别人的网站，非常惊艳，想照个做一个。看博主的自我介绍，他用的是基于next自己修改过的主题。 真的很不错，直接拿过来用就行。但是配置的时候自己功力不够，出了很多问题，比如设置搜索框的时候就一直没成功过。由于是别人自己修改过的主题，用的人不多，资料比较少，所以无奈只能放弃。 于是，只能找那种开源的很多人用过的主题，这样资料多一点，有问题比较好求助。网上用的比较多的是这一款。直接clone到themes文件夹下就行了。 最终看到了这一款，十分惊艳，直接入坑。这里是文档，这里是最终效果。 文档非常全，我这里就直接引用了，非常感谢原作者。 在站点_config.yml（hexo里面的那个）文件下修改theme就行了。 1theme: Butterfly 之后的站点配置教程可能不适合Butterfly主题。因为next系列主题用的是swig，而Butterfly主题用的是pug。如果你不了解swig和pug也完全没关系，通过一些教程修改过一些配置文件后，差不多就能了解个大致意思了。二者都只是生成最终html文件的一种辅助罢了。 如果想看next主题的一些教程，可以看这位大神的博客。 Hexo 搭建个人博客系列：基础建站篇 Hexo 搭建个人博客系列：写作技巧篇 Hexo 搭建个人博客系列：主题美化篇 Hexo 搭建个人博客系列：进阶设置篇 Hexo 搭建个人博客系列：部署上线篇 实战纪录：将 Hexo 站点备份到 Github 上 2、主页设置 修改主题配置文件_config.yml (下载主题里面的配置文件) 1234567menu: 主页: /||fa fa-home 归档: /archives/||fa fa-archive 标签: /tags/||fa fa-tags 目录: /categories/||fa fa-folder-open #链接: /link/||fa fa-link 关于: /about/||fa fa-heart 另外还需要执行以下操作，不然网页上访问会出问题 123hexo new page &quot;about&quot;hexo new page &quot;categories&quot;hexo new page &quot;tags&quot; 3、添加搜索功能 安装插件 npm install hexo-generator-searchdb --save 接着，我们进入站点配置文件_config.yml,在最后新增以下内容： 12345search: path: search.xml field: post format: html limit: 10000 最后，打开主题配置文件_config.yml,搜索local_search,其值改为true： 12345678# Local search# Please see doc for more details: https://jerryc.me/posts/21cfbf15/#本地搜索# ---------------local_search: enable: true labels: input_placeholder: Search for Posts hits_empty: &quot;We didn&apos;t find any results for the search: $&#123;query&#125;&quot; # if there are no result 4、添加评论功能 注册Leancloud 我们的评论系统其实是放在Leancloud上的，因此首先需要去注册一个账号 Leancloud官网，点我注册 注册完以后需要创建一个应用，名字可以随便起，然后 进入应用-&gt;设置-&gt;应用key 获取你的appid 和 appkey 如图所示： 修改主题配置文件_config.yml。 123456789101112# valine comment system. https://valine.js.orgvaline: enable: true # if you want use valine,please set this value is true appId: # leancloud application app id appKey: # leancloud application app key notify: false # valine mail notify (true/false) https://github.com/xCss/Valine/wiki verify: false # valine verify code (true/false) pageSize: 10 # comment list page size avatar: monsterid # gravatar style https://valine.js.org/#/avatar lang: zh-cn # i18n: zh-cn/en placeholder: 留下你的脚印吧... # valine comment input placeholder(like: Please leave your footprints ) guest_info: nick,mail #valine comment header inf 5、网站图标以及个人图像设置 这里图像路径如果是本地路径的话，则是相对于主题文件夹下的source而言的。 网站图标： 123# Favicon# ---------------favicon: /img/Luffy.ico 个人头像： 1234# Post info settings# ---------------# avatar: /img/avatar.pngavatar: /img/10763769_110002102000_2_1550882510273.jpg 6、社交链接 修改主题配置文件： 12345# ---------------social: fa fa-github: https://github.com/whuhit fa fa-envelope: mailto:whuhit09@gmail.com #fa fa-rss: /atom.xml 注意邮箱前面加上mailto:，这样点击邮箱那个图标就会自动调用邮箱。]]></content>
      <categories>
        <category>教程</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识Pytorch]]></title>
    <url>%2F2019%2F09%2F02%2FPytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F%E5%88%9D%E8%AF%86pytorch%2F</url>
    <content type="text"><![CDATA[1、tensor属性 每一个torch.Tensor都有以下三种属性： torch.dtype torch.device torch.layout torch.dtype PyTorch上的常用数据类型如下，共9种。 Data type dtype CPU tensor GPU tensor Size/bytes 32-bit floating torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 4 64-bit floating torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 8 16-bit floating torch.float16or torch.half torch.HalfTensor torch.cuda.HalfTensor - 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 1 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor - 16-bit integer (signed) torch.int16or torch.short torch.ShortTensor torch.cuda.ShortTensor 2 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 4 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor 8 Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor - 以上PyTorch中的数据类型和numpy中的相对应，占用字节大小也是一样的 ———————————————— torch.device Torch.device 是表现 torch.Tensor被分配的设备类型的类，其中分为’cpu’ 和 ‘cuda’两种，如果设备序号没有显示则表示此 tensor 被分配到当前设备, 比如: ‘cuda’ 等同于 ‘cuda’: X , X 为torch.cuda.current _device() 返回值 我们可以通过 tensor.device 来获取其属性，同时可以利用字符或字符+序号的方式来分配设备 12345678910111213通过字符串：&gt;&gt;&gt; torch.device(&apos;cuda:0&apos;)device(type=&apos;cuda&apos;, index=0)&gt;&gt;&gt; torch.device(&apos;cpu&apos;)device(type=&apos;cpu&apos;)&gt;&gt;&gt; torch.device(&apos;cuda&apos;) # 当前设备device(type=&apos;cuda&apos;)通过字符串和设备序号：&gt;&gt;&gt; torch.device(&apos;cuda&apos;, 0)device(type=&apos;cuda&apos;, index=0)&gt;&gt;&gt; torch.device(&apos;cpu&apos;, 0)device(type=&apos;cpu&apos;, index=0) 此外，cpu 和 cuda 设备的转换使用 ‘to’ 来实现： 12345&gt;&gt;&gt; device_cpu = torch.device(&quot;cpu&quot;) #声明cpu设备&gt;&gt;&gt; device_cuda = torch.device(&apos;cuda&apos;) #设备cuda设备&gt;&gt;&gt; data = torch.Tensor([1])&gt;&gt;&gt; data.to(device_cpu) #将数据转为cpu格式&gt;&gt;&gt; data.to(device_cuda) #将数据转为cuda格式 torch.layout torch.layout 是表现 torch.Tensor 内存分布的类，目前只支持 torch.strided. 另外一种方式torch.sparse_coo正在实验阶段。暂时不用管这个。]]></content>
      <categories>
        <category>Pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
</search>
